pip install lifelines==0.27.7


from lifelines.datasets import load_dd

data = load_dd()
data.head()


from lifelines.datasets import load_dd
from lifelines import KaplanMeierFitter

# Load the dataset
data = load_dd()

# Create an instance of KaplanMeierFitter
kmf = KaplanMeierFitter()

# Fit the data into the model
kmf.fit(durations = data['duration'], event_observed = data['observed'])

# Create an estimate of the survival function
kmf.plot_survival_function()



from lifelines import KaplanMeierFitter

# Create an instance of KaplanMeierFitter
kmf = KaplanMeierFitter()

# Fit the data into the model
kmf.fit(durations = data['duration'], event_observed = data['observed'])

# Calculate the median survival time
median_survival_time = kmf.median_survival_time_

print("The median survival time is:", median_survival_time)


get_ipython().getoutput("pip install langchain==0.0.18")
get_ipython().getoutput("pip install pydantic==1.10.9")
#!pip install OpenAI
get_ipython().getoutput("pip install huggingface-hub")


import os
#from langchain.llms import OpenAI
from langchain.llms import HuggingFaceHub

# set up the environment with respected API key
#os.environ["OPENAI_API_KEY"] = ""

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "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"

# you can choose between different llm models

# The "temperature" is a hyperparameter that controls the randomness of the model's output. A lower value (like 0.1) makes the output more deterministic, while a higher value makes it more random.
# "max_new_tokens" parameter sets a limit on the maximum number of new tokens (words/characters) that the model can generate as output.

llm = HuggingFaceHub(repo_id="tiiuae/falcon-7b-instruct",model_kwargs={"temperature": 0.1, "max_new_tokens": 600})


# you can use OpenAI GPT models
#llm = OpenAI(model_name="gpt-3.5-turbo")


text = "How read book effectively?"

print(llm(text))


from langchain import PromptTemplate


# Define the template
template = """
Give me step by step instruction in table format:

{text}
"""

# Create the prompt template object
summary_prompt = PromptTemplate(
    input_variables=["text"], # The name of the input variable
    template=template # The template string
)

# Format the prompt with some text
text = "I want to backflip"
formatted_prompt = summary_prompt.format(text=text)

# Print the formatted prompt
print(llm(formatted_prompt))
